{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import agent\n",
    "import env\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('tensorflow-vgg/')\n",
    "import vgg16\n",
    "\n",
    "data_dir = '/home/vagrant/ocm/language-learning/data'\n",
    "\n",
    "os.chdir('/home/vagrant/ocm/language-learning/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Catword', 'Dogword']\n"
     ]
    }
   ],
   "source": [
    "num_words = 2\n",
    "vocab = ['W'+str(i) for i in range(num_words)]\n",
    "\n",
    "vocab = ['Catword', 'Dogword']\n",
    "embed_dim = 2\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    # load image\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    assert (0 <= img).all() and (img <= 1.0).all()\n",
    "    # print \"Original Image Shape: \", img.shape\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = skimage.transform.resize(crop_img, (224, 224))\n",
    "    return resized_img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vagrant/ocm/language-learning/code'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sender.show_images(target, distractor)\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "img_dirs = ['cat', 'dog']\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image_activations(sess, vgg, image, placeholder):\n",
    "    #image_pl = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    batch = image.reshape((1, 224, 224, 3))\n",
    "    feed_dict = {placeholder: batch}\n",
    "    \n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        fc8 = sess.run(vgg.fc8, feed_dict=feed_dict)\n",
    "    \n",
    "    return(fc8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_image_activations(im_acts):\n",
    "    reordering = np.array(range(len(im_acts)))    \n",
    "    random.shuffle(reordering)\n",
    "    target_ind = np.argmin(reordering)\n",
    "    shuffled = im_acts[reordering]\n",
    "    return (shuffled, target_ind)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now building the learning graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished building the learning graph\n",
      "WARNING:tensorflow:From <ipython-input-8-ca526807e9b0>:8 in <module>.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "../logs\n",
      "/home/vagrant/ocm/language-learning/code/tensorflow-vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n",
      "Initializing replay memory...\n",
      "Replay memory initialized (not yet)\n",
      "Episode 0/2000last 10 interations performance  0\n",
      "word probs [ 0.48911381  0.51088625]\n",
      "word greedy probs [ 0.51411378  0.48588625]\n",
      "Image probs [ 0.49993736  0.50006264]\n",
      "epsilon greedy probs [ 0.52493733  0.47506264]\n",
      "cat -1.0\n",
      "Episode 1/2000word probs [ 0.41992822  0.58007187]\n",
      "word greedy probs [ 0.44492823  0.55507183]\n",
      "Image probs [ 0.50018454  0.49981546]\n",
      "epsilon greedy probs [ 0.47518453  0.52481544]\n",
      "dog 1.0\n",
      "Episode 2/2000word probs [ 0.36668563  0.63331437]\n",
      "word greedy probs [ 0.39168563  0.60831434]\n",
      "Image probs [ 0.50141847  0.49858147]\n",
      "epsilon greedy probs [ 0.47641847  0.52358145]\n",
      "cat -1.0\n",
      "Episode 3/2000word probs [ 0.31510124  0.68489873]\n",
      "word greedy probs [ 0.34010124  0.6598987 ]\n",
      "Image probs [ 0.49423188  0.50576812]\n",
      "epsilon greedy probs [ 0.51923186  0.48076811]\n",
      "dog -1.0\n",
      "Episode 4/2000word probs [ 0.28797868  0.71202135]\n",
      "word greedy probs [ 0.31297868  0.68702132]\n",
      "Image probs [ 0.49499682  0.50500321]\n",
      "epsilon greedy probs [ 0.51999682  0.48000321]\n",
      "cat 1.0\n",
      "Episode 5/2000word probs [ 0.25952265  0.74047732]\n",
      "word greedy probs [ 0.28452265  0.71547729]\n",
      "Image probs [ 0.50478888  0.49521106]\n",
      "epsilon greedy probs [ 0.47978887  0.52021104]\n",
      "dog -1.0\n",
      "Episode 6/2000word probs [ 0.24325356  0.75674647]\n",
      "word greedy probs [ 0.26825356  0.73174644]\n",
      "Image probs [ 0.45877954  0.54122049]\n",
      "epsilon greedy probs [ 0.48377955  0.51622045]\n",
      "dog 1.0\n",
      "Episode 7/2000word probs [ 0.22492468  0.77507526]\n",
      "word greedy probs [ 0.24992469  0.75007522]\n",
      "Image probs [ 0.48994637  0.51005363]\n",
      "epsilon greedy probs [ 0.51494634  0.48505363]\n",
      "dog 1.0\n",
      "Episode 8/2000word probs [ 0.22489294  0.77510709]\n",
      "word greedy probs [ 0.24989295  0.75010705]\n",
      "Image probs [ 0.48033682  0.51966321]\n",
      "epsilon greedy probs [ 0.50533682  0.49466321]\n",
      "dog 1.0\n",
      "Episode 9/2000word probs [ 0.22135586  0.7786442 ]\n",
      "word greedy probs [ 0.24635586  0.75364417]\n",
      "Image probs [ 0.5466429   0.45335704]\n",
      "epsilon greedy probs [ 0.52164286  0.47835705]\n",
      "cat -1.0\n",
      "Episode 10/2000last 10 interations performance  0.0\n",
      "word probs [ 0.22161669  0.77838331]\n",
      "word greedy probs [ 0.24661669  0.75338328]\n",
      "Image probs [ 0.4868966   0.51310337]\n",
      "epsilon greedy probs [ 0.51189661  0.48810336]\n",
      "cat 1.0\n",
      "Episode 11/2000word probs [ 0.21855897  0.78144097]\n",
      "word greedy probs [ 0.24355897  0.75644094]\n",
      "Image probs [ 0.50723171  0.49276829]\n",
      "epsilon greedy probs [ 0.48223171  0.51776826]\n",
      "dog -1.0\n",
      "Episode 12/2000word probs [ 0.21903817  0.78096187]\n",
      "word greedy probs [ 0.24403818  0.75596184]\n",
      "Image probs [ 0.49885368  0.50114632]\n",
      "epsilon greedy probs [ 0.52385366  0.47614631]\n",
      "cat -1.0\n",
      "Episode 13/2000word probs [ 0.22264104  0.77735895]\n",
      "word greedy probs [ 0.24764104  0.75235891]\n",
      "Image probs [ 0.48394045  0.51605952]\n",
      "epsilon greedy probs [ 0.50894046  0.49105951]\n",
      "dog 1.0\n",
      "Episode 14/2000word probs [ 0.23563884  0.7643612 ]\n",
      "word greedy probs [ 0.26063883  0.73936117]\n",
      "Image probs [ 0.50163662  0.49836344]\n",
      "epsilon greedy probs [ 0.47663662  0.52336341]\n",
      "dog 1.0\n",
      "Episode 15/2000word probs [ 0.24451157  0.7554884 ]\n",
      "word greedy probs [ 0.26951158  0.73048836]\n",
      "Image probs [ 0.48833913  0.51166093]\n",
      "epsilon greedy probs [ 0.5133391   0.48666093]\n",
      "cat 1.0\n",
      "Episode 16/2000word probs [ 0.24930267  0.75069731]\n",
      "word greedy probs [ 0.27430266  0.72569728]\n",
      "Image probs [ 0.49550265  0.50449735]\n",
      "epsilon greedy probs [ 0.52050263  0.47949734]\n",
      "cat -1.0\n",
      "Episode 17/2000word probs [ 0.25708923  0.74291074]\n",
      "word greedy probs [ 0.28208923  0.71791071]\n",
      "Image probs [ 0.50193381  0.49806619]\n",
      "epsilon greedy probs [ 0.47693381  0.52306616]\n",
      "cat 1.0\n",
      "Episode 18/2000word probs [ 0.26061094  0.73938906]\n",
      "word greedy probs [ 0.28561094  0.71438903]\n",
      "Image probs [ 0.4915536  0.5084464]\n",
      "epsilon greedy probs [ 0.51655358  0.48344639]\n",
      "cat -1.0\n",
      "Episode 19/2000word probs [ 0.26746395  0.73253596]\n",
      "word greedy probs [ 0.29246396  0.70753592]\n",
      "Image probs [ 0.32503715  0.67496288]\n",
      "epsilon greedy probs [ 0.35003716  0.64996284]\n",
      "dog 1.0\n",
      "Episode 20/2000last 10 interations performance  2.0\n",
      "word probs [ 0.28306666  0.71693331]\n",
      "word greedy probs [ 0.30806667  0.69193327]\n",
      "Image probs [ 0.47181907  0.52818096]\n",
      "epsilon greedy probs [ 0.49681908  0.50318092]\n",
      "cat 1.0\n",
      "Episode 21/2000word probs [ 0.29362711  0.70637292]\n",
      "word greedy probs [ 0.31862712  0.68137288]\n",
      "Image probs [ 0.46766439  0.53233564]\n",
      "epsilon greedy probs [ 0.4926644  0.5073356]\n",
      "dog -1.0\n",
      "Episode 22/2000word probs [ 0.3074587  0.6925413]\n",
      "word greedy probs [ 0.3324587   0.66754127]\n",
      "Image probs [ 0.46708313  0.5329169 ]\n",
      "epsilon greedy probs [ 0.49208313  0.50791687]\n",
      "dog 1.0\n",
      "Episode 23/2000"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "agents = agent.Agents(vocab, image_embedding_dim = 10, embedding_dim = 10, temperature=10)\n",
    "game = env.Environment(data_dir, img_dirs, 2)\n",
    "\n",
    "logs_path = os.path.join('..','logs')\n",
    "\n",
    "writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "print(logs_path)\n",
    "\n",
    "## Run the iterations of the game\n",
    "iterations = 2000\n",
    "num_classes = len(img_dirs)\n",
    "\n",
    "replay_memory_size = 50\n",
    "sender_replay_memory = []\n",
    "reciever_replay_memory = []\n",
    "replay_memory = []\n",
    "init_replay_size = 20\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "update_estimators_every = 50\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=(tf.GPUOptions(per_process_gpu_memory_fraction=0.7)))) as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    \n",
    "    image_pl = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    vgg.build(image_pl)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print('Initializing replay memory...')\n",
    "    for n in range(init_replay_size):\n",
    "        ## Sender replay\n",
    "        continue\n",
    "        t,d = game.get_images()\n",
    "        ta = get_image_activations(sess, vgg, t, image_pl)\n",
    "        da = get_image_activations(sess, vgg, d, image_pl)        \n",
    "        cw = sender.show_images(sess, ta, da)\n",
    "        im_acts = np.array([ta, da])\n",
    "        shuffled, target_ind = shuffle_image_activations(im_acts)\n",
    "        selected = reciever.show_images(sess, cw, shuffled[0], shuffled[1])\n",
    "        score = 0\n",
    "        if selected==target_ind:\n",
    "            score = 1\n",
    "        replay_memory.append((ta, da, cw, score))\n",
    "    \n",
    "    print('Replay memory initialized (not yet)')\n",
    "        \n",
    "    \n",
    "    tot_reward = 0\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        print(\"\\rEpisode {}/{}\".format(i, iterations), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('last 10 interations performance ', tot_reward)\n",
    "            tot_reward = 0\n",
    "                    \n",
    "        target_image, distractor_image = game.get_images()\n",
    "        target_class = game.target_class\n",
    "        #print('the target class is ', target_class)\n",
    "        target_acts = get_image_activations(sess, vgg, target_image, image_pl)\n",
    "        distractor_acts = get_image_activations(sess, vgg, distractor_image, image_pl)\n",
    "        \n",
    "        reordering = np.array([0,1])\n",
    "        random.shuffle(reordering)\n",
    "        target = np.where(reordering==0)[0]\n",
    "        \n",
    "        img_array = [target_acts, distractor_acts] \n",
    "        i1, i2 = [img_array[reordering[i]] for i, img in enumerate(img_array)]\n",
    "\n",
    "        shuffled_acts = np.concatenate([i1, i2], axis=0)\n",
    "        \n",
    "        ## for Sender - take action in reinforcement learning terms\n",
    "        \n",
    "        reward, word_text = agents.show_images(sess, shuffled_acts, target, target_class)\n",
    "        \n",
    "        #word = 0\n",
    "        #if target_class=='dog':\n",
    "        #    word = 1\n",
    "        \n",
    "        #reward, word_text = agents.test_sender(sess, shuffled_acts, target, target_class)\n",
    "        \n",
    "        print(target_class, reward)\n",
    "        #reward = agents.test_receiver(sess, shuffled_acts, word, target_ind, target_class)\n",
    "        tot_reward += reward\n",
    "        selection = 0\n",
    "        #agents.call_trial(sess, img_array, target_ind)\n",
    "\n",
    "        if target == selection:\n",
    "            wins += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "\n",
    "    print(wins, losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
