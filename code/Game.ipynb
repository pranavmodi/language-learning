{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import agent\n",
    "import env\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('tensorflow-vgg/')\n",
    "import vgg16\n",
    "\n",
    "data_dir = '/home/vagrant/ocm/language-learning/data'\n",
    "\n",
    "os.chdir('/home/vagrant/ocm/language-learning/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W0', 'W1']\n"
     ]
    }
   ],
   "source": [
    "num_words = 2\n",
    "vocab = ['W'+str(i) for i in range(num_words)]\n",
    "embed_dim = 2\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-519541e8469d>:3 in <module>.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "../logs\n"
     ]
    }
   ],
   "source": [
    "logs_path = os.path.join('..','logs')\n",
    "\n",
    "writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "print(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    # load image\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    assert (0 <= img).all() and (img <= 1.0).all()\n",
    "    # print \"Original Image Shape: \", img.shape\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = skimage.transform.resize(crop_img, (224, 224))\n",
    "    return resized_img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vagrant/ocm/language-learning/code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sender.show_images(target, distractor)\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "img_dirs = ['cat', 'dog']\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image_activations(sess, vgg, image, placeholder):\n",
    "    #image_pl = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    batch = image.reshape((1, 224, 224, 3))\n",
    "    feed_dict = {placeholder: batch}\n",
    "    \n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        fc8 = sess.run(vgg.fc8, feed_dict=feed_dict)\n",
    "    \n",
    "    return(fc8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_image_activations(im_acts):\n",
    "    reordering = np.array(range(len(im_acts)))    \n",
    "    random.shuffle(reordering)\n",
    "    target_ind = np.argmin(reordering)\n",
    "    shuffled = im_acts[reordering]\n",
    "    return (shuffled, target_ind)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"sender/Variable/read:0\", shape=(2000, 2), dtype=float32)'] and loss Tensor(\"sender/Sum:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5bf1819ebc58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSenderAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreciever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecieverAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vagrant/ocm/language-learning/code/agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, embedding_dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#self._build_image_to_vocab_mapper()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_learning_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vagrant/ocm/language-learning/code/agent.py\u001b[0m in \u001b[0;36m_trial_learning_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSPropOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.00025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vagrant/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    274\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"sender/Variable/read:0\", shape=(2000, 2), dtype=float32)'] and loss Tensor(\"sender/Sum:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sender = agent.SenderAgent(vocab)\n",
    "reciever = agent.RecieverAgent(vocab)\n",
    "game = env.Environment(data_dir, img_dirs, 2)\n",
    "\n",
    "## Run the iterations of the game\n",
    "iterations = 10\n",
    "num_classes = len(img_dirs)\n",
    "\n",
    "replay_memory_size = 50\n",
    "sender_replay_memory = []\n",
    "reciever_replay_memory = []\n",
    "replay_memory = []\n",
    "init_replay_size = 20\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "update_estimators_every = 50\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=(tf.GPUOptions(per_process_gpu_memory_fraction=0.7)))) as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    \n",
    "    image_pl = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    vgg.build(image_pl)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print('Initializing replay memory...')\n",
    "    for n in range(init_replay_size):\n",
    "        ## Sender replay\n",
    "        t,d = game.get_images()\n",
    "        ta = get_image_activations(sess, vgg, t, image_pl)\n",
    "        da = get_image_activations(sess, vgg, d, image_pl)        \n",
    "        cw = sender.show_images(sess, ta, da)\n",
    "        im_acts = np.array([ta, da])\n",
    "        shuffled, target_ind = shuffle_image_activations(im_acts)\n",
    "        selected = reciever.show_images(sess, cw, shuffled[0], shuffled[1])\n",
    "        score = 0\n",
    "        if selected==target_ind:\n",
    "            score = 1\n",
    "        replay_memory.append((ta, da, cw, score))\n",
    "    \n",
    "    print('Replay memory initialized')\n",
    "        \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        print(\"\\rEpisode {}/{}\".format(i, iterations), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        ## Update the sender and reciever parameters periodically\n",
    "        if i % update_estimators_every == 0:\n",
    "            print('hello')\n",
    "                    \n",
    "        target_image, distractor_image = game.get_images()\n",
    "        target_acts = get_image_activations(sess, vgg, target_image, image_pl)\n",
    "        distractor_acts = get_image_activations(sess, vgg, distractor_image, image_pl)\n",
    "        \n",
    "        ## for Sender - take action in reinforcement learning terms\n",
    "        comm_word = sender.show_images(sess, target_acts, distractor_acts)\n",
    "\n",
    "        reordering = np.array([0,1])\n",
    "        random.shuffle(reordering)\n",
    "        target_ind = np.where(reordering==0)[0]\n",
    "\n",
    "        img_array = [target_acts, distractor_acts]\n",
    "        i1, i2 = [img_array[reordering[i]] for i, img in enumerate(img_array)]\n",
    "        \n",
    "        ## For Reciever - Take action\n",
    "        selection = reciever.show_images(sess, comm_word, i1, i2) \n",
    "\n",
    "        if target_ind == selection:\n",
    "            wins += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "\n",
    "    print(wins, losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
