{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import agent\n",
    "import env\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('tensorflow-vgg/')\n",
    "import vgg16\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "data_dir = '/home/vagrant/ocm/language-learning/data'\n",
    "\n",
    "os.chdir('/home/vagrant/ocm/language-learning/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    # load image\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    assert (0 <= img).all() and (img <= 1.0).all()\n",
    "    # print \"Original Image Shape: \", img.shape\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = skimage.transform.resize(crop_img, (224, 224))\n",
    "    return resized_img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vagrant/ocm/language-learning/code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sender.show_images(target, distractor)\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "img_dirs = ['cat', 'dog']\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image_activations(sess, vgg, image, placeholder):\n",
    "    #image_pl = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    batch = image.reshape((1, 224, 224, 3))\n",
    "    feed_dict = {placeholder: batch}\n",
    "    \n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        fc8 = sess.run(vgg.fc8, feed_dict=feed_dict)\n",
    "    \n",
    "    return(fc8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_image_activations(im_acts):\n",
    "    reordering = np.array(range(len(im_acts)))    \n",
    "    random.shuffle(reordering)\n",
    "    target_ind = np.argmin(reordering)\n",
    "    shuffled = im_acts[reordering]\n",
    "    return (shuffled, target_ind)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W0', 'W1']\n"
     ]
    }
   ],
   "source": [
    "num_words = 2\n",
    "vocab = ['W'+str(i) for i in range(num_words)]\n",
    "\n",
    "#vocab = ['Catword', 'Dogword']\n",
    "embed_dim = 2\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now building the learning graph\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"sender/Shape_5:0\", shape=(2,), dtype=int32)\n",
      "word Tensor(\"word:0\", shape=(1, ?), dtype=int32)\n",
      "selected word probs Tensor(\"sender/Print_2:0\", shape=(1, ?), dtype=float32)\n",
      "reward Tensor(\"sender/Print_3:0\", shape=(?, 1), dtype=float32)\n",
      "sender loss Tensor(\"sender/mul_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created the scalar for tensorboard\n",
      "finished building the learning graph\n",
      "/home/vagrant/ocm/language-learning/code/tensorflow-vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n",
      "Episode 0/20000last 10 interations performance  0\n",
      "word probs [ 0.50055498  0.49944499]\n",
      "image probs [ 0.5  0.5]\n",
      "dog -1.0\n",
      "Episode 1/20000word probs [ 0.50036162  0.49963832]\n",
      "image probs [ 0.50000006  0.49999997]\n",
      "cat -1.0\n",
      "Episode 2/20000word probs [ 0.50053233  0.49946764]\n",
      "image probs [ 0.49999991  0.50000006]\n",
      "dog 1.0\n",
      "Episode 3/20000word probs [ 0.50053543  0.49946463]\n",
      "image probs [ 0.50000006  0.49999994]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "dog 1.0\n",
      "Episode 4/20000word probs [ 0.50979716  0.49020284]\n",
      "image probs [ 0.49614486  0.50385511]\n",
      "cat -1.0\n",
      "Episode 5/20000word probs [ 0.50204158  0.49795842]\n",
      "image probs [ 0.49857455  0.50142539]\n",
      "dog -1.0\n",
      "Episode 6/20000word probs [ 0.50204158  0.49795842]\n",
      "image probs [ 0.50150192  0.49849808]\n",
      "dog 1.0\n",
      "Episode 7/20000word probs [ 0.50092018  0.49907982]\n",
      "image probs [ 0.5000549   0.49994516]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "dog -1.0\n",
      "Episode 8/20000word probs [ 0.56147689  0.4385232 ]\n",
      "image probs [ 0.51817989  0.48182008]\n",
      "dog 1.0\n",
      "Episode 9/20000word probs [ 0.60331792  0.39668211]\n",
      "image probs [ 0.50046629  0.49953374]\n",
      "dog 1.0\n",
      "Episode 10/20000last 10 interations performance  0.0\n",
      "word probs [ 0.60331792  0.39668211]\n",
      "image probs [ 0.51303989  0.4869602 ]\n",
      "cat 1.0\n",
      "Episode 11/20000word probs [ 0.60331792  0.39668211]\n",
      "image probs [ 0.50984097  0.490159  ]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "dog 1.0\n",
      "Episode 12/20000word probs [ 0.50672078  0.49327931]\n",
      "image probs [ 0.49389625  0.50610381]\n",
      "dog -1.0\n",
      "Episode 13/20000word probs [ 0.58406508  0.41593489]\n",
      "image probs [ 0.45718896  0.54281104]\n",
      "cat 1.0\n",
      "Episode 14/20000word probs [ 0.58293122  0.41706881]\n",
      "image probs [ 0.5297845   0.47021547]\n",
      "cat -1.0\n",
      "Episode 15/20000word probs [ 0.58293122  0.41706881]\n",
      "image probs [ 0.50854099  0.49145901]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "cat 1.0\n",
      "Episode 16/20000word probs [ 0.50531435  0.49468568]\n",
      "image probs [ 0.49418384  0.50581616]\n",
      "dog -1.0\n",
      "Episode 17/20000word probs [ 0.56377459  0.43622541]\n",
      "image probs [ 0.49642336  0.50357658]\n",
      "cat 1.0\n",
      "Episode 18/20000word probs [ 0.5645659   0.43543407]\n",
      "image probs [ 0.47369304  0.52630693]\n",
      "cat 1.0\n",
      "Episode 19/20000word probs [ 0.50531435  0.49468568]\n",
      "image probs [ 0.50290716  0.49709278]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "dog 1.0\n",
      "Episode 20/20000last 10 interations performance  4.0\n",
      "word probs [ 0.5092442   0.49075577]\n",
      "image probs [ 0.54868567  0.45131436]\n",
      "cat 1.0\n",
      "Episode 21/20000word probs [ 0.49174216  0.50825787]\n",
      "image probs [ 0.46604785  0.53395218]\n",
      "dog -1.0\n",
      "Episode 22/20000word probs [ 0.54768896  0.4523111 ]\n",
      "image probs [ 0.56453526  0.43546468]\n",
      "cat 1.0\n",
      "Episode 23/20000word probs [ 0.5097661   0.49023387]\n",
      "image probs [ 0.4984023  0.5015977]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "cat -1.0\n",
      "Episode 24/20000word probs [ 0.49332061  0.50667936]\n",
      "image probs [ 0.52405131  0.47594872]\n",
      "dog 1.0\n",
      "Episode 25/20000word probs [ 0.49332061  0.50667936]\n",
      "image probs [ 0.57996237  0.42003769]\n",
      "dog 1.0\n",
      "Episode 26/20000word probs [ 0.53106308  0.46893683]\n",
      "image probs [ 0.51782453  0.48217547]\n",
      "dog 1.0\n",
      "Episode 27/20000word probs [ 0.53106308  0.46893683]\n",
      "image probs [ 0.47423279  0.52576727]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "cat -1.0\n",
      "Episode 28/20000word probs [ 0.56089044  0.43910953]\n",
      "image probs [ 0.47246575  0.52753425]\n",
      "cat -1.0\n",
      "Episode 29/20000word probs [ 0.49529648  0.50470352]\n",
      "image probs [ 0.41459909  0.58540094]\n",
      "dog 1.0\n",
      "Episode 30/20000last 10 interations performance  2.0\n",
      "word probs [ 0.56089044  0.43910953]\n",
      "image probs [ 0.35575712  0.64424288]\n",
      "cat 1.0\n",
      "Episode 31/20000word probs [ 0.49529648  0.50470352]\n",
      "image probs [ 0.48177621  0.51822376]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "dog 1.0\n",
      "Episode 32/20000word probs [ 0.49986753  0.50013256]\n",
      "image probs [ 0.53453875  0.46546125]\n",
      "dog 1.0\n",
      "Episode 33/20000word probs [ 0.49986753  0.50013256]\n",
      "image probs [ 0.53652239  0.46347761]\n",
      "dog -1.0\n",
      "Episode 34/20000word probs [ 0.49986753  0.50013256]\n",
      "image probs [ 0.55572081  0.44427916]\n",
      "dog 1.0\n",
      "Episode 35/20000word probs [ 0.5393815   0.46061847]\n",
      "image probs [ 0.52638781  0.47361222]\n",
      "updating the agent weights\n",
      "shape of the rewards  (4, 1)\n",
      "shape of the selected word  (1, 4)\n",
      "cat -1.0\n",
      "Episode 36/20000word probs [ 0.56424034  0.43575966]\n",
      "image probs [ 0.38716948  0.61283052]\n",
      "dog -1.0\n",
      "Episode 37/20000word probs [ 0.5983724   0.40162766]\n",
      "image probs [ 0.30384517  0.69615483]\n",
      "cat 1.0\n",
      "Episode 38/20000"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "agents = agent.Agents(vocab, image_embedding_dim = 10, embedding_dim = 10, temperature=10)\n",
    "game = env.Environment(data_dir, img_dirs, 2)\n",
    "\n",
    "logs_path = os.path.join('..','logs/run3')\n",
    "writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "## Run the iterations of the game\n",
    "iterations = 20000\n",
    "mini_batch_size = 4\n",
    "\n",
    "num_classes = len(img_dirs)\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "update_estimators_every = 50\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=(tf.GPUOptions(per_process_gpu_memory_fraction=0.7)))) as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    \n",
    "    image_pl = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    vgg.build(image_pl)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    batch = []\n",
    "    Game = namedtuple(\"Game\", [\"im_acts\", \"target_acts\", \"distractor_acts\", \"word_probs\", \"image_probs\", \"target\", \"word\", \"selection\", \"reward\"])\n",
    "    tot_reward = 0\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        print(\"\\rEpisode {}/{}\".format(i, iterations), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('last 10 interations performance ', tot_reward)\n",
    "            tot_reward = 0\n",
    "                    \n",
    "        target_image, distractor_image = game.get_images()\n",
    "        target_class = game.target_class\n",
    "        target_acts = get_image_activations(sess, vgg, target_image, image_pl)\n",
    "        distractor_acts = get_image_activations(sess, vgg, distractor_image, image_pl)\n",
    "        \n",
    "        reordering = np.array([0,1])\n",
    "        random.shuffle(reordering)\n",
    "        target = np.where(reordering==0)[0]\n",
    "        \n",
    "        img_array = [target_acts, distractor_acts] \n",
    "        i1, i2 = [img_array[reordering[i]] for i, img in enumerate(img_array)]\n",
    "\n",
    "        shuffled_acts = np.concatenate([i1, i2], axis=1)\n",
    "        \n",
    "        ## for Sender - take action in reinforcement learning terms\n",
    "        \n",
    "        reward, word, selection, word_probs, image_probs = agents.show_images(sess, shuffled_acts, target_acts, distractor_acts, target, target_class)\n",
    "\n",
    "        batch.append(Game(shuffled_acts, target_acts, distractor_acts, word_probs, image_probs, target, word, selection, reward))\n",
    "        \n",
    "        if len(batch) > mini_batch_size:\n",
    "            batch.pop(0)\n",
    "\n",
    "        if (i+1) % mini_batch_size == 0:\n",
    "            print('updating the agent weights')\n",
    "            summary = agents.update(sess, batch)\n",
    "            writer.add_summary(summary, i)\n",
    "            \n",
    "        #reward, word_text = agents.test_sender(sess, shuffled_acts, target, target_class)\n",
    "        print(target_class, reward)\n",
    "        #reward = agents.test_receiver(sess, shuffled_acts, word, target_ind, target_class)\n",
    "        tot_reward += reward\n",
    "        selection = 0\n",
    "        #agents.call_trial(sess, img_array, target_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
